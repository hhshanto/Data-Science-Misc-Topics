{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensemble learning is a general meta approach to machine learning that seeks better predictive performance by combining the predictions from multiple models.\n",
    "\n",
    "The three main classes of ensemble learning methods are bagging, stacking, and boosting, and it is important to both have a detailed understanding of each method and to consider them on your predictive modeling project.\n",
    "* Bagging involves fitting many decision trees on different samples of the same dataset and averaging the predictions.\n",
    "* Stacking involves fitting many different models types on the same data and using another model to learn how to best combine the predictions.\n",
    "* Boosting involves adding ensemble members sequentially that correct the predictions made by prior models and outputs a weighted average of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason ensemble learning is efficient is that your machine learning models work differently. Each model might perform well on some data and less accurately on others. When you combine all them, they cancel out each other’s weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does ensemble learning work?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to develop a machine learning model that predicts inventory stock orders for our company based on historical data we have gathered from previous years. We have used four machine learning models using a different algorithms: linear regression, support vector machine, a regression decision tree, and a basic artificial neural network. But even after much tweaking and configuration, none of them achieves your desired 95 percent prediction accuracy. These machine learning models are called “weak learners” because they fail to converge to the desired level.\n",
    "\n",
    "But weak doesn’t mean useless. You can combine them into an **ensemble**. For each new prediction, you run your input data through all four models, and then compute the average of the results. When examining the new result, you see that the aggregate results provide 96 percent accuracy, which is more than acceptable.\n",
    "\n",
    "The reason ensemble learning is efficient is that each machine learning model works differently. Each model might perform well on some data and less accurately on others. When we combine all them, they cancel out each other’s weaknesses.\n",
    "\n",
    "We can apply ensemble methods to both predictions problems, like the inventory prediction, and classification problems, such as determining whether a picture contains a certain object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
